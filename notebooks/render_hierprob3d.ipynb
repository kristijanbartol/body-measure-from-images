{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "For usage from within Docker container:\n",
    "1. Start docker container on host with argument `-p 8888:8888`\n",
    "2. From within docker:\n",
    "    - Install jupyter notebook: `pip install notebook`\n",
    "    - Start jupyter notebook: `jupyter notebook --ip 0.0.0.0 --no-browser --allow-root`\n",
    "3. On the host visit `localhost:8888/tree`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendered images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/kristijan/kristijan-hdd-ex/ShapeFromImages/.venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 1.4.0\n",
      "CUDA available\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "verbosity = 2\n",
    "if verbosity > 0:\n",
    "    print(f\"Torch {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA available\")\n",
    "    else:\n",
    "        print(\"CUDA unavailable\")\n",
    "    print(f\"Device: {device}\")\n",
    "import sys\n",
    "sys.path.append('/media/kristijan/kristijan-hdd-ex/ShapeFromImages/')\n",
    "\n",
    "from data.on_the_fly_smpl_train_dataset import OnTheFlySMPLTrainDataset\n",
    "from configs import paths\n",
    "from configs.poseMF_shapeGaussian_net_config import get_poseMF_shapeGaussian_cfg_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libtorch_cpu.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_265548/443929993.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrenderers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch3d_textured_renderer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTexturedIUVRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeshes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTexturesVertex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTexturesUV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/kristijan/kristijan-hdd-ex/ShapeFromImages/renderers/pytorch3d_textured_renderer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Data structures and functions for rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeshes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from pytorch3d.renderer import (\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mlook_at_view_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mFoVOrthographicCameras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/kristijan/kristijan-hdd-ex/ShapeFromImages/.venv/lib/python3.7/site-packages/pytorch3d/renderer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from .blending import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mBlendParams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhard_rgb_blend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/kristijan/kristijan-hdd-ex/ShapeFromImages/.venv/lib/python3.7/site-packages/pytorch3d/renderer/blending.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# pyre-fixme[21]: Could not find name `_C` in `pytorch3d`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libtorch_cpu.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from renderers.pytorch3d_textured_renderer import TexturedIUVRenderer\n",
    "import pytorch3d\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import TexturesVertex, TexturesUV\n",
    "from pytorch3d.io import IO\n",
    "if verbosity > 0:\n",
    "    print(f\"PyTorch3D {pytorch3d.__version__}\")\n",
    "from models.smpl_official import SMPL\n",
    "from smplx.lbs import batch_rodrigues\n",
    "from utils.augmentation.smpl_augmentation import normal_sample_shape\n",
    "from utils.augmentation.cam_augmentation import augment_cam_t\n",
    "from utils.augmentation.rgb_augmentation import augment_rgb\n",
    "from utils.augmentation.lighting_augmentation import augment_light\n",
    "from utils.rigid_transform_utils import aa_rotate_rotmats_pytorch3d, aa_rotate_translate_points_pytorch3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_shape_cfg = get_poseMF_shapeGaussian_cfg_defaults()\n",
    "if verbosity > 1:\n",
    "    print(f\"Configuration: \\n{pose_shape_cfg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Default Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = torch.tensor([1., 0., 0.], device=device, dtype=torch.float32)\n",
    "delta_betas_std_vector = torch.ones(\n",
    "    pose_shape_cfg.MODEL.NUM_SMPL_BETAS,\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ") * pose_shape_cfg.TRAIN.SYNTH_DATA.AUGMENT.SMPL.SHAPE_STD\n",
    "mean_shape = torch.zeros(\n",
    "    pose_shape_cfg.MODEL.NUM_SMPL_BETAS,\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "mean_cam_t = torch.tensor(\n",
    "    pose_shape_cfg.TRAIN.SYNTH_DATA.MEAN_CAM_T,\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "mean_cam_t = mean_cam_t[None, :].expand(pose_shape_cfg.TRAIN.BATCH_SIZE, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OnTheFlySMPLTrainDataset(\n",
    "    poses_path=paths.TRAIN_POSES_PATH,\n",
    "    textures_path=paths.TRAIN_TEXTURES_PATH,\n",
    "    backgrounds_dir_path=paths.TRAIN_BACKGROUNDS_PATH,\n",
    "    params_from='not_amass',\n",
    "    img_wh=pose_shape_cfg.DATA.PROXY_REP_SIZE\n",
    ")\n",
    "\n",
    "val_dataset = OnTheFlySMPLTrainDataset(\n",
    "    poses_path=paths.VAL_POSES_PATH,\n",
    "    textures_path=paths.VAL_TEXTURES_PATH,\n",
    "    backgrounds_dir_path=paths.VAL_BACKGROUNDS_PATH,\n",
    "    params_from='all',\n",
    "    img_wh=pose_shape_cfg.DATA.PROXY_REP_SIZE\n",
    ")\n",
    "if verbosity > 0:\n",
    "    print(\"Training:\")\n",
    "    print(f\"    Poses found: {len(train_dataset)}\")\n",
    "    print(f\"    Textures found (gray / non-gray): {len(train_dataset.grey_textures)} / {len(train_dataset.nongrey_textures)}\")\n",
    "    print(f\"    Backgrounds found: {len(train_dataset.backgrounds_paths)}\")\n",
    "    print(\"Validation:\")\n",
    "    print(f\"    Poses found: {len(val_dataset)}\")\n",
    "    print(f\"    Textures found (gray / non-gray): {len(val_dataset.grey_textures)} / {len(val_dataset.nongrey_textures)}\")\n",
    "    print(f\"    Backgrounds found: {len(val_dataset.backgrounds_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=pose_shape_cfg.TRAIN.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=pose_shape_cfg.TRAIN.NUM_WORKERS,\n",
    "    pin_memory=pose_shape_cfg.TRAIN.PIN_MEMORY\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=pose_shape_cfg.TRAIN.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=pose_shape_cfg.TRAIN.NUM_WORKERS,\n",
    "    pin_memory=pose_shape_cfg.TRAIN.PIN_MEMORY\n",
    ")\n",
    "\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize PyTorch3D Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TexturedIUVRenderer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-7-846ea29c85d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m pytorch3d_renderer = TexturedIUVRenderer(\n",
      "\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpose_shape_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m     \u001b[0mimg_wh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpose_shape_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROXY_REP_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m     \u001b[0mprojection_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'perspective'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TexturedIUVRenderer' is not defined"
     ]
    }
   ],
   "source": [
    "pytorch3d_renderer = TexturedIUVRenderer(\n",
    "    device=device,\n",
    "    batch_size=pose_shape_cfg.TRAIN.BATCH_SIZE,\n",
    "    img_wh=pose_shape_cfg.DATA.PROXY_REP_SIZE,\n",
    "    projection_type='perspective',\n",
    "    perspective_focal_length=pose_shape_cfg.TRAIN.SYNTH_DATA.FOCAL_LENGTH,\n",
    "    render_rgb=True,\n",
    "    bin_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SMPL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_model = SMPL(paths.SMPL, num_betas=pose_shape_cfg.MODEL.NUM_SMPL_BETAS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    target_poses = batch['pose'].to(device)\n",
    "    backgrounds = batch['background'].to(device)\n",
    "    textures = batch['texture'].to(device)\n",
    "    if verbosity > 0:\n",
    "        print(f\"Poses shape: {target_poses.shape}\")\n",
    "        print(f\"Backgrounds shape: {backgrounds.shape}\")\n",
    "        print(f\"Textures shape: {textures.shape}\")\n",
    "    break\n",
    "# Randomly sample body shape\n",
    "target_shapes = normal_sample_shape(\n",
    "    batch_size=pose_shape_cfg.TRAIN.BATCH_SIZE,\n",
    "    mean_shape=mean_shape,\n",
    "    std_vector=delta_betas_std_vector\n",
    ")\n",
    "# Randomly sample camera translation\n",
    "target_cam_t = augment_cam_t(\n",
    "    mean_cam_t,\n",
    "    xy_std=pose_shape_cfg.TRAIN.SYNTH_DATA.AUGMENT.CAM.XY_STD,\n",
    "    delta_z_range=pose_shape_cfg.TRAIN.SYNTH_DATA.AUGMENT.CAM.DELTA_Z_RANGE\n",
    ")\n",
    "if verbosity > 0:\n",
    "    print(f\"Shapes shape: {target_shapes.shape}\")\n",
    "    print(f\"Camera translation shape: {target_cam_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbosity > 1:\n",
    "    batch_size = batch['pose'].shape[0]\n",
    "    fig, axs = plt.subplots(batch_size, 2, figsize=(10,20))\n",
    "    for idx in range(batch_size):\n",
    "        axs[idx, 0].imshow(backgrounds[idx].permute(1,2,0).cpu())\n",
    "        axs[idx, 1].imshow(textures[idx].cpu())\n",
    "        axs[idx, 0].set_title(f\"Background Sample {idx}\")\n",
    "        axs[idx, 1].set_title(f\"Texture Sample {idx}\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Axis-Angle Representation to Rotation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_poses_rotmats = batch_rodrigues(target_poses.contiguous().view(-1, 3)).view(-1, 24, 3, 3)\n",
    "# first entry is global orientation\n",
    "target_glob_rotmats = target_poses_rotmats[:, 0, :, :]\n",
    "target_poses_rotmats = target_poses_rotmats[:, 1:, :, :]\n",
    "if verbosity > 1:\n",
    "    print(f\"Poses rotation matrices: {target_poses_rotmats.shape}\")\n",
    "    print(f\"Global rotation matrix: {target_glob_rotmats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flip Pose Targets such that they are right way up in 3D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, target_glob_rotmats = aa_rotate_rotmats_pytorch3d(\n",
    "    rotmats=target_glob_rotmats,\n",
    "    angles=np.pi,\n",
    "    axes=x_axis,\n",
    "    rot_mult_order='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Target Vertices and Joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_smpl_output = smpl_model(\n",
    "    body_pose=target_poses_rotmats,\n",
    "    global_orient=target_glob_rotmats.unsqueeze(1),\n",
    "    betas=target_shapes,\n",
    "    pose2rot=False\n",
    ")\n",
    "target_vertices = target_smpl_output.vertices\n",
    "target_joints_all = target_smpl_output.joints\n",
    "target_joints_h36m = target_joints_all[:, ALL_JOINTS_TO_H36M_MAP, :]\n",
    "target_joints_h36mlsp = target_joints_h36m[:, H36M_TO_J14, :]\n",
    "if verbosity > 1:\n",
    "    print(f\"Vertices: {target_vertices.shape}\")\n",
    "    print(f\"All joints: {target_joints_all.shape}\")\n",
    "    print(f\"Human3.6M joints: {target_joints_h36m.shape}\")\n",
    "    print(f\"Human3.6M LSP joints: {target_joints_h36mlsp.shape}\")\n",
    "    \n",
    "target_reposed_vertices = smpl_model(\n",
    "    body_pose=torch.zeros_like(target_poses)[:, 3:], # Removes global orientation\n",
    "    global_orient=torch.zeros_like(target_poses)[:, :3],\n",
    "    betas=target_shapes    \n",
    ").vertices\n",
    "if verbosity > 1:\n",
    "    print(f\"Reposed vertices: {target_reposed_vertices.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vertices_for_rendering = aa_rotate_translate_points_pytorch3d(\n",
    "    points=target_vertices,\n",
    "    axes=x_axis,\n",
    "    angles=np.pi,\n",
    "    translations=torch.zeros(3, device=device).float()\n",
    ")\n",
    "target_joints_coco = aa_rotate_translate_points_pytorch3d(\n",
    "    points=target_joints_all[:, ALL_JOINTS_TO_COCO_MAP, :],\n",
    "    axes=x_axis,\n",
    "    angles=np.pi,\n",
    "    translations=torch.zeros(3, device=device).float()\n",
    ")\n",
    "target_joints2d_coco = perspective_project_torch(\n",
    "    target_joints_coco,\n",
    "    None,\n",
    "    target_cam_t,\n",
    "    focal_length=pose_shape_cfg.TRAIN.SYNTH_DATA.FOCAL_LENGTH,\n",
    "    img_wh=pose_shape_cfg.DATA.PROXY_REP_SIZE\n",
    ")\n",
    "# Check if joints within image dimensions before cropping + recentering.\n",
    "target_joints2d_visib_coco = check_joints2d_visibility_torch(\n",
    "    target_joints2d_coco,\n",
    "    pose_shape_cfg.DATA.PROXY_REP_SIZE\n",
    ")\n",
    "if verbosity > 1:\n",
    "    print(f\"Vertices for rendering: {target_vertices_for_rendering.shape}\")\n",
    "    print(f\"COCO joints: {target_joints_coco.shape}\")\n",
    "    print(f\"COCO 2D joints: {target_joints2d_coco.shape}\")\n",
    "    print(f\"2D joints visible: {target_joints2d_visib_coco.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Lighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lights_rgb_settings = augment_light(\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    rgb_augment_config=pose_shape_cfg.TRAIN.SYNTH_DATA.AUGMENT.RGB\n",
    ")\n",
    "print(lights_rgb_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer_output = pytorch3d_renderer(\n",
    "    vertices=target_vertices_for_rendering,\n",
    "    textures=textures,\n",
    "    cam_t=target_cam_t,\n",
    "    lights_rgb_settings=lights_rgb_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Renders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbosity > 1:\n",
    "    batch_size = renderer_output['rgb_images'].shape[0]\n",
    "    fig, axs = plt.subplots(batch_size, 3, figsize=(10,15))\n",
    "    for idx in range(batch_size):\n",
    "        axs[idx, 0].imshow(renderer_output['rgb_images'][idx].detach().cpu())\n",
    "        axs[idx, 0].set_title('RGB')\n",
    "        axs[idx, 1].imshow(renderer_output['depth_images'][idx].detach().cpu())\n",
    "        axs[idx, 1].set_title('Depth')\n",
    "        axs[idx, 2].imshow(renderer_output['iuv_images'][idx].detach().cpu())\n",
    "        axs[idx, 2].set_title('IUV')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.6 ('.venv': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/media/kristijan/kristijan-hdd-ex/ShapeFromImages/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "verbosity = 2\n",
    "if verbosity > 0:\n",
    "    print(f\"Torch {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA available\")\n",
    "    else:\n",
    "        print(\"CUDA unavailable\")\n",
    "    print(f\"Device: {device}\")\n",
    "import sys\n",
    "sys.path.append('/garmentor')\n",
    "from data.on_the_fly_smpl_train_dataset import OnTheFlySMPLTrainDataset\n",
    "from configs import paths\n",
    "from configs.poseMF_shapeGaussian_net_config import get_poseMF_shapeGaussian_cfg_defaults\n",
    "from renderers.pytorch3d_textured_renderer import TexturedIUVRenderer\n",
    "import pytorch3d\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import TexturesVertex, TexturesUV\n",
    "from pytorch3d.io import IO\n",
    "if verbosity > 0:\n",
    "    print(f\"PyTorch3D {pytorch3d.__version__}\")\n",
    "from models.smpl_official import SMPL\n",
    "from smplx.lbs import batch_rodrigues\n",
    "from utils.augmentation.smpl_augmentation import normal_sample_shape\n",
    "from utils.augmentation.cam_augmentation import augment_cam_t\n",
    "from utils.augmentation.rgb_augmentation import augment_rgb\n",
    "from utils.augmentation.lighting_augmentation import augment_light\n",
    "from utils.rigid_transform_utils import aa_rotate_rotmats_pytorch3d, aa_rotate_translate_points_pytorch3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_shape_cfg = get_poseMF_shapeGaussian_cfg_defaults()\n",
    "if verbosity > 1:\n",
    "    print(f\"Configuration: \\n{pose_shape_cfg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Default Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = torch.tensor([1., 0., 0.], device=device, dtype=torch.float32)\n",
    "delta_betas_std_vector = torch.ones(\n",
    "    pose_shape_cfg.MODEL.NUM_SMPL_BETAS,\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ") * pose_shape_cfg.TRAIN.SYNTH_DATA.AUGMENT.SMPL.SHAPE_STD\n",
    "mean_shape = torch.zeros(\n",
    "    pose_shape_cfg.MODEL.NUM_SMPL_BETAS,\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "mean_cam_t = torch.tensor(\n",
    "    pose_shape_cfg.TRAIN.SYNTH_DATA.MEAN_CAM_T,\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "mean_cam_t = mean_cam_t[None, :].expand(pose_shape_cfg.TRAIN.BATCH_SIZE, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OnTheFlySMPLTrainDataset(\n",
    "    poses_path=paths.TRAIN_POSES_PATH,\n",
    "    textures_path=paths.TRAIN_TEXTURES_PATH,\n",
    "    backgrounds_dir_path=paths.TRAIN_BACKGROUNDS_PATH,\n",
    "    params_from='not_amass',\n",
    "    img_wh=pose_shape_cfg.DATA.PROXY_REP_SIZE\n",
    ")\n",
    "\n",
    "val_dataset = OnTheFlySMPLTrainDataset(\n",
    "    poses_path=paths.VAL_POSES_PATH,\n",
    "    textures_path=paths.VAL_TEXTURES_PATH,\n",
    "    backgrounds_dir_path=paths.VAL_BACKGROUNDS_PATH,\n",
    "    params_from='all',\n",
    "    img_wh=pose_shape_cfg.DATA.PROXY_REP_SIZE\n",
    ")\n",
    "if verbosity > 0:\n",
    "    print(\"Training:\")\n",
    "    print(f\"    Poses found: {len(train_dataset)}\")\n",
    "    print(f\"    Textures found (gray / non-gray): {len(train_dataset.grey_textures)} / {len(train_dataset.nongrey_textures)}\")\n",
    "    print(f\"    Backgrounds found: {len(train_dataset.backgrounds_paths)}\")\n",
    "    print(\"Validation:\")\n",
    "    print(f\"    Poses found: {len(val_dataset)}\")\n",
    "    print(f\"    Textures found (gray / non-gray): {len(val_dataset.grey_textures)} / {len(val_dataset.nongrey_textures)}\")\n",
    "    print(f\"    Backgrounds found: {len(val_dataset.backgrounds_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=pose_shape_cfg.TRAIN.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=pose_shape_cfg.TRAIN.NUM_WORKERS,\n",
    "    pin_memory=pose_shape_cfg.TRAIN.PIN_MEMORY\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=pose_shape_cfg.TRAIN.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=pose_shape_cfg.TRAIN.NUM_WORKERS,\n",
    "    pin_memory=pose_shape_cfg.TRAIN.PIN_MEMORY\n",
    ")\n",
    "\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize PyTorch3D Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch3d_renderer = TexturedIUVRenderer(\n",
    "    device=device,\n",
    "    batch_size=pose_shape_cfg.TRAIN.BATCH_SIZE,\n",
    "    img_wh=pose_shape_cfg.DATA.PROXY_REP_SIZE,\n",
    "    projection_type='perspective',\n",
    "    perspective_focal_length=pose_shape_cfg.TRAIN.SYNTH_DATA.FOCAL_LENGTH,\n",
    "    render_rgb=True,\n",
    "    bin_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SMPL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl_model = SMPL(paths.SMPL, num_betas=pose_shape_cfg.MODEL.NUM_SMPL_BETAS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    target_poses = batch['pose'].to(device)\n",
    "    backgrounds = batch['background'].to(device)\n",
    "    textures = batch['texture'].to(device)\n",
    "    if verbosity > 0:\n",
    "        print(f\"Poses shape: {target_poses.shape}\")\n",
    "        print(f\"Backgrounds shape: {backgrounds.shape}\")\n",
    "        print(f\"Textures shape: {textures.shape}\")\n",
    "    break\n",
    "# Randomly sample body shape\n",
    "target_shapes = normal_sample_shape(\n",
    "    batch_size=pose_shape_cfg.TRAIN.BATCH_SIZE,\n",
    "    mean_shape=mean_shape,\n",
    "    std_vector=delta_betas_std_vector\n",
    ")\n",
    "# Randomly sample camera translation\n",
    "target_cam_t = augment_cam_t(\n",
    "    mean_cam_t,\n",
    "    xy_std=pose_shape_cfg.TRAIN.SYNTH_DATA.AUGMENT.CAM.XY_STD,\n",
    "    delta_z_range=pose_shape_cfg.TRAIN.SYNTH_DATA.AUGMENT.CAM.DELTA_Z_RANGE\n",
    ")\n",
    "if verbosity > 0:\n",
    "    print(f\"Shapes shape: {target_shapes.shape}\")\n",
    "    print(f\"Camera translation shape: {target_cam_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbosity > 1:\n",
    "    batch_size = batch['pose'].shape[0]\n",
    "    fig, axs = plt.subplots(batch_size, 2, figsize=(10,20))\n",
    "    for idx in range(batch_size):\n",
    "        axs[idx, 0].imshow(backgrounds[idx].permute(1,2,0).cpu())\n",
    "        axs[idx, 1].imshow(textures[idx].cpu())\n",
    "        axs[idx, 0].set_title(f\"Background Sample {idx}\")\n",
    "        axs[idx, 1].set_title(f\"Texture Sample {idx}\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Axis-Angle Representation to Rotation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_poses_rotmats = batch_rodrigues(target_poses.contiguous().view(-1, 3)).view(-1, 24, 3, 3)\n",
    "# first entry is global orientation\n",
    "target_glob_rotmats = target_poses_rotmats[:, 0, :, :]\n",
    "target_poses_rotmats = target_poses_rotmats[:, 1:, :, :]\n",
    "if verbosity > 1:\n",
    "    print(f\"Poses rotation matrices: {target_poses_rotmats.shape}\")\n",
    "    print(f\"Global rotation matrix: {target_glob_rotmats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flip Pose Targets such that they are right way up in 3D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, target_glob_rotmats = aa_rotate_rotmats_pytorch3d(\n",
    "    rotmats=target_glob_rotmats,\n",
    "    angles=np.pi,\n",
    "    axes=x_axis,\n",
    "    rot_mult_order='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Target Vertices and Joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_smpl_output = smpl_model(\n",
    "    body_pose=target_poses_rotmats,\n",
    "    global_orient=target_glob_rotmats.unsqueeze(1),\n",
    "    betas=target_shapes,\n",
    "    pose2rot=False\n",
    ")\n",
    "target_vertices = target_smpl_output.vertices\n",
    "target_joints_all = target_smpl_output.joints\n",
    "target_joints_h36m = target_joints_all[:, ALL_JOINTS_TO_H36M_MAP, :]\n",
    "target_joints_h36mlsp = target_joints_h36m[:, H36M_TO_J14, :]\n",
    "if verbosity > 1:\n",
    "    print(f\"Vertices: {target_vertices.shape}\")\n",
    "    print(f\"All joints: {target_joints_all.shape}\")\n",
    "    print(f\"Human3.6M joints: {target_joints_h36m.shape}\")\n",
    "    print(f\"Human3.6M LSP joints: {target_joints_h36mlsp.shape}\")\n",
    "    \n",
    "target_reposed_vertices = smpl_model(\n",
    "    body_pose=torch.zeros_like(target_poses)[:, 3:], # Removes global orientation\n",
    "    global_orient=torch.zeros_like(target_poses)[:, :3],\n",
    "    betas=target_shapes    \n",
    ").vertices\n",
    "if verbosity > 1:\n",
    "    print(f\"Reposed vertices: {target_reposed_vertices.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vertices_for_rendering = aa_rotate_translate_points_pytorch3d(\n",
    "    points=target_vertices,\n",
    "    axes=x_axis,\n",
    "    angles=np.pi,\n",
    "    translations=torch.zeros(3, device=device).float()\n",
    ")\n",
    "target_joints_coco = aa_rotate_translate_points_pytorch3d(\n",
    "    points=target_joints_all[:, ALL_JOINTS_TO_COCO_MAP, :],\n",
    "    axes=x_axis,\n",
    "    angles=np.pi,\n",
    "    translations=torch.zeros(3, device=device).float()\n",
    ")\n",
    "target_joints2d_coco = perspective_project_torch(\n",
    "    target_joints_coco,\n",
    "    None,\n",
    "    target_cam_t,\n",
    "    focal_length=pose_shape_cfg.TRAIN.SYNTH_DATA.FOCAL_LENGTH,\n",
    "    img_wh=pose_shape_cfg.DATA.PROXY_REP_SIZE\n",
    ")\n",
    "# Check if joints within image dimensions before cropping + recentering.\n",
    "target_joints2d_visib_coco = check_joints2d_visibility_torch(\n",
    "    target_joints2d_coco,\n",
    "    pose_shape_cfg.DATA.PROXY_REP_SIZE\n",
    ")\n",
    "if verbosity > 1:\n",
    "    print(f\"Vertices for rendering: {target_vertices_for_rendering.shape}\")\n",
    "    print(f\"COCO joints: {target_joints_coco.shape}\")\n",
    "    print(f\"COCO 2D joints: {target_joints2d_coco.shape}\")\n",
    "    print(f\"2D joints visible: {target_joints2d_visib_coco.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Lighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lights_rgb_settings = augment_light(\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    rgb_augment_config=pose_shape_cfg.TRAIN.SYNTH_DATA.AUGMENT.RGB\n",
    ")\n",
    "print(lights_rgb_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer_output = pytorch3d_renderer(\n",
    "    vertices=target_vertices_for_rendering,\n",
    "    textures=textures,\n",
    "    cam_t=target_cam_t,\n",
    "    lights_rgb_settings=lights_rgb_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Renders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbosity > 1:\n",
    "    batch_size = renderer_output['rgb_images'].shape[0]\n",
    "    fig, axs = plt.subplots(batch_size, 3, figsize=(10,15))\n",
    "    for idx in range(batch_size):\n",
    "        axs[idx, 0].imshow(renderer_output['rgb_images'][idx].detach().cpu())\n",
    "        axs[idx, 0].set_title('RGB')\n",
    "        axs[idx, 1].imshow(renderer_output['depth_images'][idx].detach().cpu())\n",
    "        axs[idx, 1].set_title('Depth')\n",
    "        axs[idx, 2].imshow(renderer_output['iuv_images'][idx].detach().cpu())\n",
    "        axs[idx, 2].set_title('IUV')\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90a6017628d5cdce0d44e5e7bbd52e05eb69906c7b94fdb212071e3d1603a9ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
