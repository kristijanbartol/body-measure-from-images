{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from smplx.lbs import batch_rodrigues\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "import sys\n",
    "sys.path.append('/ShapeFromImages/')\n",
    "sys.path.append('/media/kristijan/kristijan-hdd-ex/ShapeFromImages/')\n",
    "\n",
    "from PointRend.point_rend import add_pointrend_config\n",
    "\n",
    "from utils.image_utils import pad_to_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPO_DIR = '/ShapeFromImages/'\n",
    "REPO_DIR = '/media/kristijan/kristijan-hdd-ex/ShapeFromImages/'\n",
    "IMG_DIR = os.path.join(REPO_DIR, 'demo')\n",
    "RESULT_DIR = os.path.join(IMG_DIR, 'result/')\n",
    "INPUT_WH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_detectron2_predictors():\n",
    "    # Keypoint-RCNN\n",
    "    kprcnn_config_file = \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"\n",
    "    kprcnn_cfg = get_cfg()\n",
    "    kprcnn_cfg.merge_from_file(model_zoo.get_config_file(kprcnn_config_file))\n",
    "    kprcnn_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "    kprcnn_cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(kprcnn_config_file)\n",
    "    kprcnn_cfg.freeze()\n",
    "    joints2D_predictor = DefaultPredictor(kprcnn_cfg)\n",
    "\n",
    "    # PointRend-RCNN-R50-FPN\n",
    "    pointrend_config_file = os.path.join(REPO_DIR, 'PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml')\n",
    "    pointrend_cfg = get_cfg()\n",
    "    add_pointrend_config(pointrend_cfg)\n",
    "    pointrend_cfg.merge_from_file(pointrend_config_file)\n",
    "    pointrend_cfg.MODEL.WEIGHTS = os.path.join(REPO_DIR, 'checkpoints/pointrend_rcnn_R_50_fpn.pkl')\n",
    "    pointrend_cfg.freeze()\n",
    "    silhouette_predictor = DefaultPredictor(pointrend_cfg)\n",
    "\n",
    "    return joints2D_predictor, silhouette_predictor\n",
    "\n",
    "\n",
    "def get_largest_centred_bounding_box(bboxes, orig_w, orig_h):\n",
    "    \"\"\"\n",
    "    Given an array of bounding boxes, return the index of the largest + roughly-centred\n",
    "    bounding box.\n",
    "    :param bboxes: (N, 4) array of [x1 y1 x2 y2] bounding boxes\n",
    "    :param orig_w: original image width\n",
    "    :param orig_h: original image height\n",
    "    \"\"\"\n",
    "    bboxes_area = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])\n",
    "    sorted_bbox_indices = np.argsort(bboxes_area)[::-1]  # Indices of bboxes sorted by area.\n",
    "    bbox_found = False\n",
    "    i = 0\n",
    "    while not bbox_found and i < sorted_bbox_indices.shape[0]:\n",
    "        bbox_index = sorted_bbox_indices[i]\n",
    "        bbox = bboxes[bbox_index]\n",
    "        bbox_centre = ((bbox[0] + bbox[2]) / 2.0, (bbox[1] + bbox[3]) / 2.0)  # Centre (width, height)\n",
    "        if abs(bbox_centre[0] - orig_w / 2.0) < orig_w/6.0 and abs(bbox_centre[1] - orig_h / 2.0) < orig_w/6.0:\n",
    "            largest_centred_bbox_index = bbox_index\n",
    "            bbox_found = True\n",
    "        i += 1\n",
    "\n",
    "    # If can't find bbox sufficiently close to centre, just use biggest bbox as prediction\n",
    "    if not bbox_found:\n",
    "        largest_centred_bbox_index = sorted_bbox_indices[0]\n",
    "\n",
    "    return largest_centred_bbox_index\n",
    "\n",
    "\n",
    "def predict_joints2D(input_image, predictor):\n",
    "    \"\"\"\n",
    "    Predicts 2D joints (17 2D joints in COCO convention along with prediction confidence)\n",
    "    given a cropped and centred input image.\n",
    "    :param input_images: (wh, wh)\n",
    "    :param predictor: instance of detectron2 DefaultPredictor class, created with the\n",
    "    appropriate config file.\n",
    "    \"\"\"\n",
    "    image = np.copy(input_image)\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    outputs = predictor(image)  # Multiple bboxes + keypoints predictions if there are multiple people in the image\n",
    "    bboxes = outputs['instances'].pred_boxes.tensor.cpu().numpy()\n",
    "    if bboxes.shape[0] == 0:  # Can't find any people in image\n",
    "        keypoints = np.zeros((17, 3))\n",
    "    else:\n",
    "        largest_centred_bbox_index = get_largest_centred_bounding_box(bboxes, orig_w, orig_h)  # Picks out centred person that is largest in the image.\n",
    "        keypoints = outputs['instances'].pred_keypoints.cpu().numpy()\n",
    "        keypoints = keypoints[largest_centred_bbox_index]\n",
    "        \n",
    "        print(keypoints.dtype)\n",
    "\n",
    "        for j in range(keypoints.shape[0]):\n",
    "            cv2.circle(image, (int(keypoints[j, 0]), int(keypoints[j, 1])), 5, (0, 255, 0), -1)\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontScale = 0.5\n",
    "            fontColor = (0, 0, 255)\n",
    "            cv2.putText(image, str(j), (int(keypoints[j, 0]), int(keypoints[j, 1])),\n",
    "                                     font, fontScale, fontColor, lineType=2)\n",
    "\n",
    "    return keypoints, image\n",
    "\n",
    "\n",
    "def get_largest_centred_mask(human_masks, orig_w, orig_h):\n",
    "    \"\"\"\n",
    "    Given an array of human segmentation masks, return the index of the largest +\n",
    "    roughly-centred mask.\n",
    "    :param human_masks: (N, img_wh, img_wh) human segmentation masks.\n",
    "    :param orig_w: original image width\n",
    "    :param orig_h: original image height\n",
    "    \"\"\"\n",
    "    mask_areas = np.sum(human_masks, axis=(1, 2))\n",
    "    sorted_mask_indices = np.argsort(mask_areas)[::-1]  # Indices of masks sorted by area.\n",
    "    mask_found = False\n",
    "    i = 0\n",
    "    while not mask_found and i < sorted_mask_indices.shape[0]:\n",
    "        mask_index = sorted_mask_indices[i]\n",
    "        mask = human_masks[mask_index, :, :]\n",
    "        mask_pixels = np.argwhere(mask != 0)\n",
    "        bbox_corners = np.amin(mask_pixels, axis=0), np.amax(mask_pixels, axis=0)  # (row_min, col_min), (row_max, col_max)\n",
    "        bbox_centre = ((bbox_corners[0][0] + bbox_corners[1][0]) / 2.0,\n",
    "                       (bbox_corners[0][1] + bbox_corners[1][1]) / 2.0)  # Centre in rows, columns (i.e. height, width)\n",
    "\n",
    "        if abs(bbox_centre[0] - orig_h / 2.0) < orig_w/4.0 and abs(bbox_centre[1] - orig_w / 2.0) < orig_w/6.0:\n",
    "            largest_centred_mask_index = mask_index\n",
    "            mask_found = True\n",
    "        i += 1\n",
    "\n",
    "    # If can't find mask sufficiently close to centre, just use biggest mask as prediction\n",
    "    if not mask_found:\n",
    "        largest_centred_mask_index = sorted_mask_indices[0]\n",
    "\n",
    "    return largest_centred_mask_index\n",
    "\n",
    "\n",
    "def predict_silhouette_pointrend(input_image, predictor):\n",
    "    \"\"\"\n",
    "    Predicts human silhouette (binary segmetnation) given a cropped and centred input image.\n",
    "    :param input_images: (wh, wh)\n",
    "    :param predictor: instance of detectron2 DefaultPredictor class, created with the\n",
    "    appropriate config file.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = input_image.shape[:2]\n",
    "    outputs = predictor(input_image)['instances']  # Multiple silhouette predictions if there are multiple people in the image\n",
    "    classes = outputs.pred_classes\n",
    "    masks = outputs.pred_masks\n",
    "    human_masks = masks[classes == 0]\n",
    "    human_masks = human_masks.cpu().detach().numpy()\n",
    "    largest_centred_mask_index = get_largest_centred_mask(human_masks, orig_w, orig_h)  # Picks out centred person that is largest in the image.\n",
    "    human_mask = human_masks[largest_centred_mask_index, :, :].astype(np.uint8)\n",
    "    overlay_vis = cv2.addWeighted(input_image, 1.0,\n",
    "                              255 * np.tile(human_mask[:, :, None], [1, 1, 3]),\n",
    "                              0.5, gamma=0)\n",
    "\n",
    "    return human_mask, overlay_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fnames = [f for f in sorted(os.listdir(IMG_DIR)) if f.endswith('.png') or\n",
    "                        f.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up proxy representation predictors.\n",
    "joints2D_predictor, silhouette_predictor = setup_detectron2_predictors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on: 0000.png\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "Predicting on: 0001.png\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "Predicting on: 0002.png\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "Predicting on: 0003.png\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "Predicting on: 0004.png\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "Predicting on: 0005.png\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "Predicting on: 0006.png\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n",
      "Predicting on: 0007.png\n",
      "<class 'numpy.ndarray'>\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "for fname in image_fnames:\n",
    "    print(\"Predicting on:\", fname)\n",
    "    image = cv2.imread(os.path.join(IMG_DIR, fname))\n",
    "    \n",
    "    # Preprocess for 2D detectors.\n",
    "    image = pad_to_square(image)\n",
    "    image = cv2.resize(image, (INPUT_WH, INPUT_WH),\n",
    "                       interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    print(type(image))\n",
    "    \n",
    "    # Predict joints.\n",
    "    joints2D, joints2D_vis = predict_joints2D(image, joints2D_predictor)\n",
    "\n",
    "    # Predict silhouette.\n",
    "    silhouette, silhouette_vis = predict_silhouette_pointrend(image,\n",
    "                                                              silhouette_predictor)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(RESULT_DIR, f'{fname}_joints.png'), joints2D_vis)\n",
    "    cv2.imwrite(os.path.join(RESULT_DIR, f'{fname}_silh.png'), silhouette_vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
